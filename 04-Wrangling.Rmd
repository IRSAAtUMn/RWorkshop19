# Understanding data: Transformations


*Author: Alicia Johnson*



\
\


## Getting started {-}

As you get settled in, please open a new Rmd (R markdown document) and prepare the data:

```{r}
# Import data from the fivethirtyeight package
library(fivethirtyeight)
data("candy_rankings")

# Rename the dataset to something shorter
candy <- candy_rankings

# Load packages
library(ggplot2)  # for visualizing data
library(dplyr)    # for transforming data
```



\
\
\
\




## Goal: Understanding Through Transformations

In this module we'll focus on the __"Transform"__ step within the data science workflow:

\
\


<center>
<div class="image">
<img src="images/data-science.png" style="width: 475px"/>
<div>source: [Wickham & Grolemund: R for Data Science](http://r4ds.had.co.nz/) </div>
</div>
</center>


\
\


In the previous modules, we imported and visualized tidy data.  These visualizations provided us with important insights into the patterns in our data.  Data transformations are also fundamental to understanding data: 

- We can utilize __data transformations__ to:    
    - define & explore new variables;    
    - filter & focus on subsets of the data; and    
    - reorder the data.

- Simple numerical __summaries__ calculated via data transformations complement, formalize, and support the patterns observed in our visual summaries.




\
\
\
\




## tidyverse: dplyr

We'll construct data transformations using the `dplyr`, a package within the broader tidyverse.  Like `ggplot`, the `dplyr` "grammar" is intuitive and generalizable once mastered.  The best way to learn about `dplyr` is to just play around.  Don't worry about memorizing the syntax.  Rather, focus on the *patterns* and *potential* of their application.  There's a helpful cheat sheet for future reference:

<center>
[**DPLYR CHEAT SHEET**](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
</center>


\
\


In the `dplyr` grammar, there are 5 _verbs_ (actions):


\


verb            action
--------------- ---------------------------------------------
`arrange()`     reorder the _rows_    
`filter()`      take a subset of _rows_    
`select()`      take a subset of _columns_    
`mutate()`      create a new variable, ie. _column_    
`summarize()`   calculate a numerical summary of a variable, i.e. _column_



\
\


The general syntax for applying these verbs is below, where we call "`%>%`" a "pipe":

```{r eval = FALSE}
my_dataset %>% 
  verb(___)
```

Just as we can add layers to a `ggplot` utilizing "`+`", we can implement sequential data transformations utilizing "`%>%":

```{r eval = FALSE}
my_dataset %>% 
  verb1(___) %>% 
  verb2(___)
```











\
\
\
\






## Transforming rows: `arrange()` and `filter()`    

\
\


The fivethirtyeight article
[The Ultimate Halloween Candy Power Ranking](http://fivethirtyeight.com/features/the-ultimate-halloween-candy-power-ranking/) analyzes the data from [this experiment](http://walthickey.com/2017/10/18/whats-the-best-halloween-candy/) which presented subjects with a series of head-to-head candy matchups and asked them to indicate which candy they preferred.  You __imported__ these __tidy__ data above and named it `candy`.    


\
\



1. __Basics__    
    ```{r}
    # Take a glimpse at the data.  What's the unit of observation?
    
    # How much data do we have?    
    
    ```
    
    
\
\


    
2. __arrange() rows__    
    Summarize what the `arrange()` function does.       
    ```{r message=FALSE, warning=FALSE, eval=FALSE}    
    # ???
    candy %>% 
      arrange(winpercent)
    candy %>% 
      arrange(winpercent) %>% 
      head()
    
    # ???
    candy %>% 
      arrange(desc(winpercent))
    candy %>% 
      arrange(desc(winpercent)) %>% 
      head() 
    
    # Store the new dataset
    candy_arranged <- candy %>% 
      arrange(winpercent)
    
    # You try: arrange from least to most expensive 
    
    
    # You try: arrange from most to least expensive 
    
    ```
    
    

\
\



4. __filter() rows__    
    We're not always interested in _all_ rows of a dataset.  For example, here we might be interested in studying only chocolate, not _all_ candy.  `filter()` allows us to keep only certain *rows* that meet a given criterion.  To write these criteria, we must specify the *variable* by which we want to filter the data and the *value(s)* of that variable that we want to keep.  Here are some general rules to try out below:    
    
    - If variable `x` is *quantitative*:    
        `x == 1`, `x < 1`, `x <= 1`, `x > 1`, `x >= 1`, `x != 1`    
        
    - If variable `x` is *categorical / factor*:    
        `x == "a"`, `x != "a"`    
        
    - If variable `x` is *logical* (`TRUE` / `FALSE`):    
        `x == TRUE`, `x == FALSE`    
    
    ```{r eval = FALSE}
    # Keep the sad candies that won less than 30% 
    candy %>% 
      filter(___)
    
    # Keep only Dots
    candy %>% 
      filter(___)
    
    # Keep only chocolate-y candies    
    candy %>% 
      filter(___)
    ```
    

\
\



5. __Combining `arrange()` and `filter()`__    
    We can perform multiple, sequential `arrange()` and `filter()` operations.    
    ```{r eval = FALSE}
    # Keep only chocolate-y, peanut butter-y candies. Do this in 3 lines.
    candy %>% 
      filter(___)  %>% 
      filter(___)
    
    # Keep only chocolate-y, peanut butter-y candies. Do this in 2 lines.
    candy %>% 
      filter(___, ___)
    
    # Arrange fruity candy from most to least popular
    candy %>% 
      ___(___) %>% 
      ___(___)
    
    # Can you rearrange the operations above to arrange fruity candy from most to least popular?
    candy %>% 
      ___(___) %>% 
      ___(___)
    ```





\
\
\
\




## Transforming columns: `select()` and `mutate()`    


6. `select()` columns    
    There are often more variables (columns) in a dataset than we're interested in.  Removing the superfluous variables can make data analysis more computationally efficient and less overwhelming.    
    ```{r eval = FALSE}
    # Create a dataset which only contains candy name & winpercent
    candy %>% 
      select(competitorname, winpercent) %>% 
      head()
    
    # You try: create a dataset which only contains candy name, chocolate status, & winpercent
    
    
    # You try: can you guess how to remove only the nougat variable?
    
    ```
    


\
\



7. `mutate()` columns    
    We can create new variables (columns) by _mutating_ existing columns.  This is helpful when we want to change the scale of a variable, combine variables into new measurements, etc.    
    
    ```{r eval = FALSE}
    # Create price_percentile variable which transforms pricepercent to 0-100 scale
    candy %>% 
      mutate(price_percentile = ___) %>% 
      head()
    
    # You try: create a meaningless_sum variable which is the sum of sugarpercent and pricepercent 

    
    ```
    
    
    
    
    

\
\
\
\




## Simple numerical summaries: `summarize()`

\
\

8. __Review: univariate visualizations__    
    Construct a univariate visualization of `winpercent`.  Summarize the trend and variability in the `winpercent` from candy to candy.        
    ```{r}
    
    ```
    

\
\


9. __Univariate numerical summaries of trend and variability__    
    The visualization above allows us to _eyeball_ the trend and variability in `winpercent`.  We can support these observations with rigorous numerical summaries.    
    ```{r eval = FALSE}
    # Trend: calculate the mean winpercent
    candy %>% 
      summarize(mean(winpercent))
    
    # Trend: calculate the mean & median winpercent
    candy %>% 
      summarize(mean(winpercent), median(winpercent))
    
    # Variability: calculate the min & max winpercent
    candy %>% 
      summarize(___, ___)
    ```
    

\
\


10. __Review: multivariate visualizations__    
    Construct a visualization of the relationship between `winpercent` and `chocolate`.  Summarize your observations.        
    ```{r}
    
    ```
    

\
\



11. __Numerical summaries by group__    
    The visualization above illuminates how the trend and variability in `winpercent` differs depending upon `chocolate` status.  We can calculate separate numerical summaries for these groups.    
    ```{r eval = FALSE}
    # Trend: calculate the mean winpercent by chocolate group
    candy %>% 
      group_by(chocolate) %>% 
      summarize(mean(winpercent))
    
    # (You try) Variability: calculate the min & max winpercent by fruit group
    
    ```
    
    



\
\
\
\





## Exercises

Practice, practice, practice is important when learning the `dplyr` verbs.  Try out the following exercises with solutions below.  Again, there are more exerices here than you will finish during the workshop.    


\
\


### Questions


1. __Revisiting movies__    
    Recall the `bechdel` data:    
    
    ```{r}
    data("bechdel")
    ```   
    
    To simplify our analysis, create a new dataset named `new_bechdel` which contains only the following variables: `title`, `year`, `binary`, `clean_test`, `budget_2013`, `domgross_2013`.  HINT: Which verb is helpful here: `arrange()`, `filter()`, `select()`, `mutate()`, `summarize()`  
    
    ```{r eval = FALSE}
    new_bechdel <- ___
    ```
   


\
\


2. __Transforming budget and gross__    
    Film budgets and profits can be huge.  To make these values easier to explore, create two new variables, `budget_mil` and `gross_mil`, which transform `budget_2013` and `domgross_2013` to the millions scale (instead of $ scale).  Be sure to store these in the `new_bechdel` dataset.  HINT: Which verb is helpful here: `arrange()`, `filter()`, `select()`, `mutate()`, `summarize()`  
    
    ```{r eval = FALSE}
    new_bechdel <- ___
    ```
    

\
\



3. __Most vs least expensive.  Most vs least profitable__    
    a. Identify the 6 movies with the largest budgets and the 6 movies with the smallest budgets.    
    b. Identify the 6 movies with the largest gross and the 6 movies with the smallest gross.    
    c. Among films that _pass_ the Bechdel test, which are the 6 with the largest gross?  HINT: You'll need to combine 2 `dplyr` verbs!



\
\


4. __Which films?__    
    Identify which films were...    
    a. Made in 2013.    
    b. Made in 2013 and didn't pass the "at least 2 women characters" criterion of the Bechdel test.  HINT: This is indicated by the `nowomen` level of the `clean_test` variable.        
    c. Made in 2013 and grossed more than $300,000,000.
    

\
\


5. __Min, Median, Max__    
    a. Among all films in the dataset, calculate the minimum, median, and maximum budget.    
    b. Among all films in the dataset, calculate and compare the minimum, median, and maximum budgets for movies that pass and fail the Bechdel test.    
    c. Among films made in the 1970s, calculate and compare the minimum, median, and maximum budgets for movies that pass and fail the Bechdel test.    
    d. Among films made in the 2010s, calculate and compare the minimum, median, and maximum budgets for movies that pass and fail the Bechdel test.    
    e. Compare your results for parts c and d.  What does this reveal about changes in the movie business?    
    
    

\
\


6. __Challenge: Friday the 13th__    
    In this final exercise, you'll need to combine some of the `dplyr` verbs in a more open-ended scenario.  Hints are provided below.  Recall the `US_births_2000_2014` births data:    
    
    ```{r}
    data("US_births_2000_2014")
    ```

    We'll use these data to determine whether there's any evidence that people are superstitious about giving birth on Friday the 13th.  Specifically:    
    
    a. Visualize the birth patterns among Fridays that fall on the 13th day of the month and Fridays that fall off the 13th day.    
    b. Calculate and compare the median number of births on Fridays that fall on the 13th vs Fridays that fall off the 13th.    
    c. Summarize your findings.  Mainly, did you find ample evidence of superstition?
    
    \
    
    
    HINTS:    
    - We're only interested in births that occur on Fridays.    
    - There's currently no variable that lumps Friday birthdays into two categories: those that fall on the 13th day of the month and those that don't.  
    

\
\
\
\

    
    
### Solutions


1. .    
    ```{r}
    data("bechdel")
    new_bechdel <- bechdel %>% 
      select(title, year, binary, clean_test, budget_2013, domgross_2013)
    ```
   


\
\


2. .    
    ```{r}
    new_bechdel <- new_bechdel %>% 
        mutate(budget_mil = budget_2013 / 1000000, gross_mil = domgross_2013 / 1000000)
    ```
    

\
\



3. .    
    ```{r}
    # a
    new_bechdel %>% 
        arrange(desc(budget_mil)) %>% 
        head()
    new_bechdel %>% 
        arrange(budget_mil) %>% 
        head()
    
    # b
    new_bechdel %>% 
        arrange(desc(gross_mil)) %>% 
        head()
    new_bechdel %>% 
        arrange(gross_mil) %>% 
        head()
    
    # c 
    new_bechdel %>% 
        filter(binary == "PASS") %>% 
        arrange(desc(gross_mil)) %>% 
        head()
    ```

    

\
\


4. .    
    ```{r}
    # a
    new_bechdel %>% 
        filter(year == 2013) %>% 
        head()   # just for simplicity
    
    # b 
    new_bechdel %>% 
        filter(year == 2013, clean_test == "nowomen")
    
    # c 
    new_bechdel %>% 
        filter(year == 2013, gross_mil > 300)
    
    
    ```
    

\
\


5. .    
    ```{r}
    # a
    new_bechdel %>% 
        summarize(min(budget_mil), median(budget_mil), max(budget_mil))
    
    # b
    new_bechdel %>% 
        group_by(binary) %>% 
        summarize(min(budget_mil), median(budget_mil), max(budget_mil))
    
    # c
    new_bechdel %>% 
        filter(year < 1980) %>% 
        group_by(binary) %>% 
        summarize(min(budget_mil), median(budget_mil), max(budget_mil))
    
    # d
    new_bechdel %>% 
        filter(year >= 2010) %>% 
        group_by(binary) %>% 
        summarize(min(budget_mil), median(budget_mil), max(budget_mil))
    ```


    

\
\


6. .    

    ```{r}
    data("US_births_2000_2014")
    
    # Filter out all but Fridays and define Friday the 13th variable
    new_births <- US_births_2000_2014 %>% 
        filter(day_of_week == "Fri") %>% 
        mutate(fri13 = (date_of_month == 13))
    
    # a
    ggplot(new_births, aes(x = births, fill = fri13)) + 
        geom_density(alpha = 0.5)
    
    # b
    new_births %>% 
        group_by(fri13) %>% 
        summarize(median(births))
    ```



\
\
\
\



## Resources

Garrett Grolemund and Hadley Wickham wrote a [free, online book](https://r4ds.had.co.nz/) that spans the tidyverse, including `ggplot2` and `dplyr`.    



    